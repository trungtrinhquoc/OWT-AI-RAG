{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Alternative ways to execute runnables\n",
    "* Invoke, Stream and Batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 004-invoke-stream-batch.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ae74d-efe9-4d20-949f-0b57df6ee289",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **004-invoke-stream-batch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662650f0-fe77-42e8-8c1a-a61aff9b72f7",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca81b4-dd76-4b4f-b714-8901d3df3e76",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5c33d0-1805-4810-95ac-394fa19ce0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "google_api_key = os.environ[\"GOOGLE_API_KEY\"]\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a04d63-0f4a-411d-94f9-decf8ed7af0e",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a004d-4c98-47ab-b720-0f09c94d42b5",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae8595e-5c07-4b02-8a79-db55fd357527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b5d23-5d09-4c88-b32f-3299795cd91b",
   "metadata": {},
   "source": [
    "## LCEL Chain\n",
    "* **An LCEL Chain is a Sequence of Runnables.**\n",
    "* Almost any component in LangChain (prompts, models, output parsers, vector store retrievers, tools, etc) can be used as a Runnable.\n",
    "* Runnables can be chained together using the pipe operator `|`. The resulting chains of runnables are also runnables themselves.\n",
    "* The order (left to right) of the elements in a LCEL chain matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5a34484-0981-4edc-8ba2-7c8521d28c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "246e32f8-4380-41d2-b33a-04ff4085569b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cristiano Ronaldo is the only player to score 50+ goals in six consecutive seasons.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player} but short and basic answer\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1026d5-2e99-4fd4-af2a-ac2c6d2eeeac",
   "metadata": {},
   "source": [
    "* All the components of the chain are Runnables.\n",
    "* **When we write chain.invoke() we are using invoke with all the componentes of the chain in an orderly manner**:\n",
    "    * First, we apply .invoke() with the user input to the prompt template.\n",
    "    * Then, with the completed prompt, we apply .invoke() to the model.\n",
    "    * And finally, we apply .invoke() to the output parser with the output of the model.\n",
    "* IMPORTANT: the order of operations in a chain matters. If you try to execute the previous chain with the components in different order, the chain will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825056a-404d-4630-b5be-77130efaf0f8",
   "metadata": {},
   "source": [
    "## Let's see this graphically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31565f6-f04f-4afb-91c6-598814fd0877",
   "metadata": {},
   "source": [
    "![alt text](./lcel-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dafb72-bea0-422b-93ba-d8a0ab244019",
   "metadata": {},
   "source": [
    "## And now let's replicate this process without using the LCEL chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6c802-2e5f-46e7-970b-5e336489c03e",
   "metadata": {},
   "source": [
    "#### First, we apply .invoke() with the user input to the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c5fbd44-6969-4d8f-ba83-ddd9e7d72193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a curious fact about Ronaldo but short and basic answer', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ab4a4-999d-4640-9a62-886e4fdc5ce2",
   "metadata": {},
   "source": [
    "#### Then, with the completed prompt, we apply .invoke() to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ede59f87-0be9-4fda-b6c4-dd521b62f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "output_after_first_step = [HumanMessage(content='tell me a curious fact about Ronaldo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7606951-561a-4679-8f59-8fdbd097734c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's a curious fact about Cristiano Ronaldo:\\n\\nDespite being known for his incredible jumping ability and powerful headers, Cristiano Ronaldo actually holds a record for the highest recorded jump by a soccer player to score a goal. He jumped 2.56 meters (8 feet 5 inches) in the air to score a header against Sampdoria in 2019 while playing for Juventus. What makes it even more impressive is that his head made contact with the ball at a height of 71 centimeters (28 inches) off the ground.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--c849e707-018b-4bba-8c25-94543353da9a-0', usage_metadata={'input_tokens': 7, 'output_tokens': 112, 'total_tokens': 119, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(output_after_first_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19cc252-e57b-49e3-9887-7f4ccacbb4de",
   "metadata": {},
   "source": [
    "#### And finally, we apply .invoke() to the output parser with the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a157968-1b54-4cf3-bc1d-d4ee4ff2b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "output_after_second_step = AIMessage(content='One curious fact about Cristiano Ronaldo is that he does not have any tattoos on his body. Despite the fact that many professional athletes have tattoos, Ronaldo has chosen to keep his body ink-free.', response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 14, 'total_tokens': 52}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c9812511-043a-458a-bfb8-005bc0d057fb-0', usage_metadata={'input_tokens': 14, 'output_tokens': 38, 'total_tokens': 52})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "074ea557-69b2-4eed-b9ba-4344d15c35d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One curious fact about Cristiano Ronaldo is that he does not have any tattoos on his body. Despite the fact that many professional athletes have tattoos, Ronaldo has chosen to keep his body ink-free.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(output_after_second_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123eadeb-a912-4c8b-b42e-af1a5f848977",
   "metadata": {},
   "source": [
    "## Ways to execute Runnables\n",
    "* Remember:\n",
    "    * An LCEL Chain is a Sequence of Runnables.\n",
    "    * Almost any component in LangChain (prompts, models, output parsers, etc) can be used as a Runnable.\n",
    "    * Runnables can be chained together using the pipe operator `|`. The resulting chains of runnables are also runnables themselves.\n",
    "    * The order (left to right) of the elements in a LCEL chain matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf49277-35e5-4061-94f0-c79270203846",
   "metadata": {},
   "source": [
    "#### LCEL Chains/Runnables are used with:\n",
    "* chain.invoke(): call the chain on an input.\n",
    "* chain.stream(): call the chain on an input and stream back chunks of the response.\n",
    "* chain.batch(): call the chain on a list of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3fe651b-dc5b-4067-a430-a4fa5da3faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me one sentence about {politician}.\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd006047-876e-4d5a-a116-e4124c4324af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Winston Churchill was a British statesman who served as Prime Minister of the United Kingdom during World War II.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--3621a2d3-7751-4070-be2a-313e0b5b665a-0', usage_metadata={'input_tokens': 7, 'output_tokens': 21, 'total_tokens': 28, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"politician\": \"Churchill\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89db19ed-7425-4506-956f-4ebb3eae44f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Franklin Delano Roosevelt led the United States through the Great Depression and most of World War II, implementing the New Deal to address economic hardship and guiding the nation as a major Allied power.\n"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"politician\": \"F.D. Roosevelt\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea804f6-d7f1-41aa-a6ef-ee668b8d0705",
   "metadata": {},
   "source": [
    "#### The for loop explained in simple terms\n",
    "This `for` loop is used to show responses piece by piece as they are received. Here's how it works in simple terms:\n",
    "\n",
    "1. **Getting Responses in Parts**: The loop receives pieces of a response from the system one at a time. In this example, the system is responding with information about the politician \"F.D. Roosevelt.\"\n",
    "\n",
    "2. **Printing Out Responses Immediately**: Each time a new piece of the response is received, it immediately prints it out. The setup makes sure there are no new lines between parts, so it all looks like one continuous response.\n",
    "\n",
    "3. **No Waiting**: By using this loop, you don't have to wait for the entire response to be ready before you start seeing parts of it. This makes it feel quicker and more like a conversation.\n",
    "\n",
    "This way, the loop helps provide a smoother and more interactive way of displaying responses from the system as they are generated.\n",
    "\n",
    "#### The for loop explained in technical terms\n",
    "This `for` loop is used to handle streaming output from a language model response. Hereâ€™s a breakdown of its functionality and context:\n",
    "\n",
    "1. **Iteration through Streamed Output**: The `for` loop iterates over the generator returned by `chain.stream(...)`. The `stream` method of the `chain` object (which is a combination of a prompt template and a language model) is designed to yield responses incrementally. This is particularly useful when responses from a model are long or need to be displayed in real-time.\n",
    "\n",
    "2. **Data Fetching**: In this loop, `s` represents each piece of content that is streamed back from the model as the response is being generated. The model in this case is set up to respond with information about the politician \"F.D. Roosevelt\".\n",
    "\n",
    "3. **Output Handling**: Inside the loop, `print(s.content, end=\"\", flush=True)` is called for each piece of streamed content. The `print` function is customized with:\n",
    "   - `end=\"\"`: This parameter ensures that each piece of content is printed without adding a new line after each one, thus allowing the response to appear continuous on a single line.\n",
    "   - `flush=True`: This parameter forces the output buffer to be flushed immediately after each print statement, ensuring that each piece of content is displayed to the user as soon as it is received without any delay.\n",
    "\n",
    "By using this loop, the code is able to display each segment of the model's response as it becomes available, making the user interface more dynamic and responsive, especially for real-time applications where prompt feedback is beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b680921b-e75f-4d77-b655-22dc9a91e9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Vladimir Lenin was a Russian revolutionary, politician, and political theorist who served as the first and founding head of government of Soviet Russia.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--ad09077f-2667-4ee4-a163-e9751f246212-0', usage_metadata={'input_tokens': 7, 'output_tokens': 27, 'total_tokens': 34, 'input_token_details': {'cache_read': 0}}),\n",
       " AIMessage(content='Joseph Stalin was the dictator of the Soviet Union from the mid-1920s until his death in 1953, known for his totalitarian rule and policies that led to widespread famine and political repression.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--0cbd6a70-9faa-497e-93b6-6a9c762b82e9-0', usage_metadata={'input_tokens': 7, 'output_tokens': 45, 'total_tokens': 52, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"politician\": \"Lenin\"}, {\"politician\": \"Stalin\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0a455-5f6b-4b32-8cef-4d6ccce13526",
   "metadata": {},
   "source": [
    "#### LCEL Chains/Runnables can be also used asynchronously:\n",
    "* chain.ainvoke(): call the chain on an input.\n",
    "* chain.astream(): call the chain on an input and stream back chunks of the response.\n",
    "* chain.abatch(): call the chain on a list of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e97ca-f124-4c0d-9565-2f60a1b92d49",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 004-invoke-stream-batch.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 004-invoke-stream-batch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feaa8ab-caa4-48a5-9a12-c14881889bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b403-RJfSVSKc-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
