{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Main built-in LCEL functions for runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46ff75-72ab-4391-904f-459bae77fc7a",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* .bind()\n",
    "* .assign()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 006-builtin-functions-for-runnables.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0fa416-0eb3-4ce2-aede-c93a89acd0de",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **006-builtin-functions-with-runnables**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2824df-acba-4fe8-8c41-4440d868c815",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b675012-0e92-442f-bc61-970b1a742115",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6ca065f-1521-415b-81da-f68eb6cb9c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "google_api_key = os.environ[\"GOOGLE_API_KEY\"]\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68972776-72f9-4d4e-95a6-41bc4eb1fc91",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e48ed-f955-4c89-b735-76c677b86e48",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ae8595e-5c07-4b02-8a79-db55fd357527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b5d23-5d09-4c88-b32f-3299795cd91b",
   "metadata": {},
   "source": [
    "## LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5a34484-0981-4edc-8ba2-7c8521d28c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "181ba6bd-cf77-43e0-8eb9-0c8e274b3051",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb069f5-558d-4494-830d-958aa4f78ef3",
   "metadata": {},
   "source": [
    "* The \"pipe\" operator `|` is the main element of the LCEL chains.\n",
    "* The order (left to right) of the elements in a LCEL chain matters.\n",
    "* An LCEL Chain is a Sequence of Runnables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "246e32f8-4380-41d2-b33a-04ff4085569b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a curious fact about Cristiano Ronaldo:\\n\\nDespite being famous for his incredible jumping ability and heading prowess, Ronaldo actually underwent surgery as a teenager to correct a heart condition called tachycardia. This condition caused his heart to race, potentially forcing him to give up football altogether. Thankfully, the surgery was successful, and he was back on the pitch within days!\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1026d5-2e99-4fd4-af2a-ac2c6d2eeeac",
   "metadata": {},
   "source": [
    "* All the components of the chain are Runnables.\n",
    "* When we write chain.invoke() we are using invoke with all the componentes of the chain in an orderly manner:\n",
    "    * First, we apply .invoke() to the prompt.\n",
    "    * Then, with the previous output, we apply .invoke() to the model.\n",
    "    * And finally, with the previous output, we apply .invoke() to the output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584cbddb-7adc-44c2-847a-bfd6e0907b10",
   "metadata": {},
   "source": [
    "## Use of .bind() to add arguments to a Runnable in a LCEL Chain\n",
    "* For example, we can add an argument to stop the model response when it reaches the word \"Ronaldo\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd686bcb-b128-4bdb-b34e-a585c1f2ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model.bind(stop=[\"Ronaldo\"]) | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f9c195f-2c4e-4b36-ba44-c4b1ec1f7d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a curious fact about Cristiano \""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a148eb-565d-4dfe-9d3c-062ed2c67468",
   "metadata": {},
   "source": [
    "## Use of .bind() to call an OpenAI Function in a LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56716892-6b51-4f03-949d-c770a6832ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions = [\n",
    "#     {\n",
    "#       \"name\": \"soccerfacts\",\n",
    "#       \"description\": \"Curious facts about a soccer player\",\n",
    "#       \"parameters\": {\n",
    "#         \"type\": \"object\",\n",
    "#         \"properties\": {\n",
    "#           \"question\": {\n",
    "#             \"type\": \"string\",\n",
    "#             \"description\": \"The question for the curious facts about a soccer player\"\n",
    "#           },\n",
    "#           \"answer\": {\n",
    "#             \"type\": \"string\",\n",
    "#             \"description\": \"The answer to the question\"\n",
    "#           }\n",
    "#         },\n",
    "#         \"required\": [\"question\", \"answer\"]\n",
    "#       }\n",
    "#     }\n",
    "#   ]\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "class SoccerFacts(BaseModel):\n",
    "    question: str = Field(description=\"The question for the curious facts about a soccer player\")\n",
    "    answer: str = Field(description=\"The answer to the question\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=SoccerFacts)\n",
    "\n",
    "# Thay vì rely vào function_call, ta ép Gemini output JSON\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Provide curious facts about soccer player {soccer_player}. \"\n",
    "    \"Return in JSON format with fields: question, answer.\\n\"\n",
    "    \"Schema: {format_instructions}\"\n",
    ").partial(format_instructions=parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70e498d3-3cf0-4948-85a3-65ea358015d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "# chain = (\n",
    "#     prompt\n",
    "#     | model.bind(function_call={\"name\": \"soccerfacts\"}, functions= functions)\n",
    "#     | JsonOutputFunctionsParser()\n",
    "# )\n",
    "chain = (\n",
    "    prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e60ea942-ce8a-4f9e-99f2-6e5d45ca0edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': \"What is Kylian Mbappé's full name?\", 'answer': 'Kylian Mbappé Lottin'}, {'question': 'What other sport did Mbappé excel at as a child?', 'answer': 'Handball. He was reportedly very talented.'}, {'question': 'How old was Mbappé when he made his professional debut for Monaco?', 'answer': '16 years and 347 days old'}, {'question': \"What is the name of Mbappé's younger brother, who is also a professional footballer?\", 'answer': 'Ethan Mbappé'}, {'question': 'What charity is Mbappé a patron of?', 'answer': 'Premiers de Cordée, which provides sports opportunities for hospitalized children and children with disabilities.'}, {'question': 'How many goals did Mbappé score in the 2018 FIFA World Cup?', 'answer': '4 goals'}, {'question': 'What record did Mbappé break at the 2018 World Cup final?', 'answer': 'He became the second teenager to score in a World Cup final, after Pelé in 1958.'}, {'question': \"What is Mbappé's estimated IQ?\", 'answer': 'While not officially confirmed, some reports suggest his IQ is around 178, putting him in the genius range.'}, {'question': \"What is the name of Mbappé's foundation?\", 'answer': 'Inspired by KM'}, {'question': \"What is unique about Mbappé's shirt number at PSG?\", 'answer': 'He wears the number 7, following in the footsteps of PSG legends like Raí and David Ginola.'}]\n"
     ]
    }
   ],
   "source": [
    "# chain.invoke(input={\"soccer_player\": \"Mbappe\"})\n",
    "result = chain.invoke({\"soccer_player\": \"Mbappe\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd1b9b-3b51-4d0a-81f1-75261bb233b6",
   "metadata": {},
   "source": [
    "**Note:** OpenAI API has deprecated functions in favor of tools. See [here](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/openai_functions_agent/) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142d874-21af-4aa0-a0b7-067a8cb75838",
   "metadata": {},
   "source": [
    "## Use of .bind() to attach OpenAI tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0d59a-f68a-43f9-b463-a06915fa808b",
   "metadata": {},
   "source": [
    "**Note:** In the OpenAI Chat API, functions are now considered a legacy options that is deprecated in favor of tools. If you're creating agents using OpenAI LLM models, you should be using OpenAI Tools rather than OpenAI functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c7d35-02e0-4924-b21b-1f9e4eafb40b",
   "metadata": {},
   "source": [
    "While you should generally use the .bind_tools() method for tool-calling models, you can also bind provider-specific args directly if you want lower level control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edbb2252-cc4c-48d2-9fe4-d612b74df56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d96079d-0597-4082-9d3a-af7c8a36f6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Los Angeles, CA\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--29f038cf-2f99-4261-8781-b5d3d52aa3cf-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA'}, 'id': 'a02f7d40-1f4f-4b25-9079-c2f98bde4b97', 'type': 'tool_call'}, {'name': 'get_current_weather', 'args': {'location': 'New York, NY'}, 'id': 'd09562e0-babf-4d4e-8067-4f995b4f5b57', 'type': 'tool_call'}, {'name': 'get_current_weather', 'args': {'location': 'Los Angeles, CA'}, 'id': 'b94a41cd-cf6b-4937-8232-36fe1a5e5fc3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 48, 'output_tokens': 30, 'total_tokens': 78, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=google_api_key).bind(tools=tools)\n",
    "model.invoke(\"What's the weather in SF, NYC and LA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6cb212-3a91-479a-829e-c17d12d76e85",
   "metadata": {},
   "source": [
    "## The assign() function allows adding keys to a chain\n",
    "* Example: we will create a key name \"operation_b\" assigned to a custom function with a RunnableLambda.\n",
    "* We will start with a very basic chain with just RunnablePassthrough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7335550-2628-468a-b687-d9aa96294d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "chain = RunnableParallel({\"original_input\": RunnablePassthrough()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ff74a9e-ef49-4a98-b5a9-3da5b5c21d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_input': 'whatever'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"whatever\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99966bbe-f9fe-4ede-b8f1-ae7d5c529c1e",
   "metadata": {},
   "source": [
    "* As you can see, right now this chain is only assigning the user input to the \"original_input\" variable.\n",
    "* Let's now add the new key \"uppercase\" with the assign function.\n",
    "* In the new \"uppercase\" key, we will use a RunnableLambda with the custom function named `make_uppercase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5498c91c-548f-4bb3-acca-90927ab5d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_uppercase(arg):\n",
    "    return arg[\"original_input\"].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a69f270-a09d-48ea-8787-c313d1f88e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel({\"original_input\": RunnablePassthrough()}).assign(uppercase=RunnableLambda(make_uppercase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99e646ef-8e1f-450f-b5cd-395affae83b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_input': 'whatever', 'uppercase': 'WHATEVER'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"whatever\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1622bcb-0bd3-4e19-bb26-cdcd5b27eabc",
   "metadata": {},
   "source": [
    "* As you can see, the output of the chain has now 2 keys: original_input and uppercase.\n",
    "* In the uppercase key, we can see that the `make_uppercase` function has been applied to the user input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7536ba98-c266-476f-af8e-6dac2fb0d2dc",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 006-builtin-functions-for-runnables.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 006-builtin-functions-for-runnables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334a856-c50b-46c4-83e6-12f7f5c1bb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b403-RJfSVSKc-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
